{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fce15ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# db: simple read test\n",
    "import json\n",
    "import yaml\n",
    "import types\n",
    "import open_clip\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "from utils.db_utils import get_df, get_data, connect_db, DB\n",
    "from utils.plotly_utils import *\n",
    "from utils.vis import *\n",
    "from utils.predict_scenegraph import PredictSceneGraph\n",
    "from utils.imagine_nav_planner import ImagineNavPlanner\n",
    "\n",
    "dump_folder = 'dump/hm3d_tradeoff_path_sigmoid_schedule'\n",
    "output_folder = f'{dump_folder}/objectnav-dino'\n",
    "\n",
    "# list db size\n",
    "! ls -lh $output_folder\n",
    "\n",
    "# load results\n",
    "results = get_df(f'{output_folder}/result.db', 'result')\n",
    "print(f'Loaded {len(results)} results')\n",
    "print(f'Current success rate: {results.tail(1)[\"success\"].values[0]/len(results):.2%}')\n",
    "print(f'Current SPL: {results[\"spl\"].mean():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ead2839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load agent modules\n",
    "device = torch.device(\"cuda\")\n",
    "args = types.SimpleNamespace(**json.load(open(f'{dump_folder}/args.json')))\n",
    "with open(f'{dump_folder}/{args.exp_config}') as f:\n",
    "    exp_config = yaml.safe_load(f)\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n",
    "    \"ViT-H-14\", \"laion2b_s32b_b79k\"\n",
    ")\n",
    "clip_model = clip_model.to(device).half()\n",
    "clip_tokenizer = open_clip.get_tokenizer(\"ViT-H-14\")\n",
    "clip_model_list = (clip_model, clip_preprocess, clip_tokenizer)\n",
    "imagine_nav_planner = ImagineNavPlanner(args, exp_config, clip_model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access db\n",
    "import os\n",
    "import pathlib\n",
    "with DB(f'{output_folder}/result.db') as con:\n",
    "    table = con.table('result')\n",
    "    print(table)\n",
    "# episode infos\n",
    "# steps_df = get_df(f'{output_folder}/result.db', 'result', select=['count_steps', 'episode', 'target', 'habitat_success', 'switch_upstair_count', 'switch_downstair_count'])\n",
    "steps_df = get_df(f'{output_folder}/result.db', 'result', filter=lambda x:x['count_steps']>490, select=['count_steps', 'episode', 'target', 'habitat_success', 'switch_upstair_count', 'switch_downstair_count'])\n",
    "print(steps_df.head(30))\n",
    "# step infos\n",
    "sample_episode_label = steps_df['episode'].values[0]\n",
    "with DB(f'{output_folder}/steps/{sample_episode_label}.db') as con:\n",
    "    table = con.table('step_data')\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e688f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sg_data(episode_label, step):\n",
    "    data = get_data(\n",
    "        f'{output_folder}/steps/{episode_label}.db',\n",
    "        'step_data',\n",
    "        filter=lambda x: (x['episode_label']==episode_label) & (x['step']==step),\n",
    "        select=[\n",
    "            'global_scene_graph',\n",
    "            'gt_scenegraph',\n",
    "            'timestamp',\n",
    "            'cate_object',\n",
    "            'origins_grid',\n",
    "            'current_grid_pose',\n",
    "            'camera_position_tensor',\n",
    "            'global_bev_rgb_map_tensor',\n",
    "            # 'occupancy_map_tensor',\n",
    "            'gradient_map_tensor',\n",
    "        ]\n",
    "    )\n",
    "    data = data[np.argmax([x['timestamp'] for x in data])]\n",
    "    return data\n",
    "\n",
    "def xyz_to_grid(xyz, grid_size, origins_grid):\n",
    "    return np.array([\n",
    "        np.floor((xyz[:, 0])*100 / grid_size).astype(int) + int(origins_grid[0]),\n",
    "        np.floor((xyz[:, 2])*100 / grid_size).astype(int) + int(origins_grid[1]),\n",
    "    ]).T\n",
    "\n",
    "def update_region(region, grid_size, origins_grid):\n",
    "    if 'bbox' in region:\n",
    "        # gt\n",
    "        bbox = xyz_to_grid(np.array(region['bbox']), grid_size, origins_grid)\n",
    "        region['grid_bbox'] = bbox\n",
    "    else:\n",
    "        # obs\n",
    "        object_centers = []\n",
    "        for obj in region.get('objects'):\n",
    "            object_centers.append(obj['center'])\n",
    "        object_centers = np.array(object_centers).astype(np.int32)\n",
    "        rect = cv2.minAreaRect(object_centers)\n",
    "        bbox = np.array(cv2.boxPoints(rect))\n",
    "        region['grid_bbox'] = bbox\n",
    "        region['center'] = np.array(rect[0])\n",
    "        if not 'caption' in region and len(region['objects']) == 0:\n",
    "            region['caption'] = 'unknown'\n",
    "    return bbox\n",
    "\n",
    "def plot_region_cv2(img, regions, map_size, show_objects=False):\n",
    "    viridis = mpl.colormaps['gist_rainbow'].resampled(len(regions))\n",
    "    colors = (viridis(range(len(regions)))[:, :3] * 255).astype(np.uint8)\n",
    "    for i, region in enumerate(regions):\n",
    "        if show_objects:\n",
    "            for obj in region.get('objects'):\n",
    "                obj_center = np.array([obj['center'][1], map_size-obj['center'][0]]).astype(np.int32)\n",
    "                cv2.circle(img, obj_center, 5, (255, 0, 0), 2)\n",
    "        pts = region['grid_bbox'].reshape((-1, 1, 2)).astype(np.int32)\n",
    "        pts[...,0], pts[...,1] = pts[...,1], map_size - pts[...,0]\n",
    "        cv2.polylines(img, [pts], isClosed=True, color=colors[i].tolist(), thickness=2)\n",
    "        region_center = np.array([region['center'][1], map_size-region['center'][0]]).astype(np.int32)\n",
    "        img = cv2.circle(img, region_center, 5, (0, 255, 0), -1)\n",
    "        img, _ = add_text(img, f\"({region['id']}: {region['caption']})\", region_center, font_scale=0.35, color=(200,200,200), thickness=1, horizontal_align='center', vertical_align='center')\n",
    "    return img\n",
    "\n",
    "\n",
    "def plot_matches_cv2(k_vis, q_vis, matches, reversed=False, map_size=480):\n",
    "    vis = np.concatenate((k_vis, q_vis), axis=1)\n",
    "    for match in matches:\n",
    "        k_center = np.array([match['k']['center'][1], map_size-match['k']['center'][0]]).astype(np.int32)\n",
    "        q_center = np.array([match['q']['center'][1], map_size-match['q']['center'][0]]).astype(np.int32)\n",
    "        if reversed:\n",
    "            k_center += np.array([map_size, 0])\n",
    "        else:\n",
    "            q_center += np.array([map_size, 0])\n",
    "        cv2.circle(vis, k_center, 5, (255, 0, 0), 2)\n",
    "        cv2.circle(vis, q_center, 5, (255, 0, 0), 2)\n",
    "        cv2.line(vis, k_center, q_center, (255, 0, 0), 2)\n",
    "        add_text(vis, f\"{match['corr_score']:.2f}\", (k_center+q_center)//2, font_scale=0.7, color=(200,200,0), thickness=2, horizontal_align='center', vertical_align='center')\n",
    "    return vis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.path import Path\n",
    "def check_overlap(region, matched_region, relaxed=False):\n",
    "    # if a[center] is in b[bbox] and b[center] is in a[bbox]\n",
    "    polygon_a = Path(region['grid_bbox'])\n",
    "    polygon_b = Path(matched_region['grid_bbox'])\n",
    "    is_a_in_b = polygon_b.contains_point(region['center'])\n",
    "    is_b_in_a = polygon_a.contains_point(matched_region['center'])\n",
    "    if relaxed:\n",
    "        return is_a_in_b or is_b_in_a\n",
    "    else:\n",
    "        return is_a_in_b and is_b_in_a\n",
    "def detect_match(scene_graph, keys, queries, knn=None, distance=None, overlap_relaxed=False, corr_score=0.5):\n",
    "    \"\"\"\n",
    "    scene_graph is just used for calculating the corr_score\n",
    "    set either knn or distance or corr_thresh to filter matches\n",
    "    overlap_relaxed: True/False/None, None means no check for region match\n",
    "    \"\"\"\n",
    "    if knn is None:\n",
    "        knn = len(keys)\n",
    "    else:\n",
    "        knn = min(knn, len(keys))\n",
    "    matches = []\n",
    "    tree = KDTree([x['center'] for x in keys])\n",
    "    for query in queries:\n",
    "        distances, indices = tree.query(query['center'], k=knn)\n",
    "        if not isinstance(distances, np.ndarray):\n",
    "            distances, indices = np.array([distances]), np.array([indices])\n",
    "        matches += [dict(k=keys[i], q=query, dist=dist) for i, dist in zip(indices, distances)]\n",
    "    if distance is not None:\n",
    "        matches = [*filter(lambda x: x['dist'] <= distance, matches)]\n",
    "    if overlap_relaxed is not None:\n",
    "        matches = [*filter(lambda x: check_overlap(x['k'], x['q'], relaxed=overlap_relaxed), matches)]\n",
    "    # calculate corr_score\n",
    "    for match in matches:\n",
    "        match['corr_score'] = scene_graph.get_text_sim_score(match['k']['caption'], match['q']['caption'])\n",
    "    if corr_score is not None:\n",
    "        matches = [*filter(lambda x: x['corr_score'] >= corr_score, matches)]\n",
    "    # keep only one match for each query according to corr_score\n",
    "    matches = sorted(matches, key=lambda x: x['corr_score'], reverse=True)\n",
    "    seen = set()\n",
    "    new_matches = []\n",
    "    for match in matches:\n",
    "        if match['q']['id'] in seen:\n",
    "            continue\n",
    "        seen.add(match['q']['id'])\n",
    "        new_matches.append(match)\n",
    "    matches = new_matches\n",
    "    return matches\n",
    "\n",
    "def evaluate_sg(\n",
    "    predict_sg,\n",
    "    obs_regions,\n",
    "    gt_regions,\n",
    "    obs_objects,\n",
    "    gt_objects,\n",
    "    knn_region = 3,\n",
    "    knn_object = 5,\n",
    "    max_object_dist = 100.0/5.0,\n",
    "):\n",
    "    matches = {}\n",
    "    matches['region_recall_relaxed'] = detect_match(\n",
    "        scene_graph=predict_sg,\n",
    "        keys=obs_regions,\n",
    "        queries=gt_regions,\n",
    "        knn=knn_region,\n",
    "        overlap_relaxed=True,\n",
    "        corr_score=None,\n",
    "    )\n",
    "\n",
    "    matches['region_recall_strict'] = detect_match(\n",
    "        scene_graph=predict_sg,\n",
    "        keys=obs_regions,\n",
    "        queries=gt_regions,\n",
    "        knn=knn_region,\n",
    "        overlap_relaxed=False,\n",
    "        corr_score=None,\n",
    "    )\n",
    "    matches['region_precision_relaxed'] = detect_match(\n",
    "        scene_graph=predict_sg,\n",
    "        keys=gt_regions,\n",
    "        queries=obs_regions,\n",
    "        knn=knn_region,\n",
    "        overlap_relaxed=True,\n",
    "        corr_score=None,\n",
    "    )\n",
    "    matches['region_precision_strict'] = detect_match(\n",
    "        scene_graph=predict_sg,\n",
    "        keys=gt_regions,\n",
    "        queries=obs_regions,\n",
    "        knn=knn_region,\n",
    "        overlap_relaxed=False,\n",
    "        corr_score=None,\n",
    "    )\n",
    "    matches['object_recall'] = detect_match(\n",
    "        scene_graph=predict_sg,\n",
    "        keys=obs_objects,\n",
    "        queries=gt_objects,\n",
    "        knn=knn_object,\n",
    "        distance=max_object_dist,\n",
    "        overlap_relaxed=None,\n",
    "        corr_score=0.9,\n",
    "    )\n",
    "    matches['object_precision'] = detect_match(\n",
    "        scene_graph=predict_sg,\n",
    "        keys=gt_objects,\n",
    "        queries=obs_objects,\n",
    "        knn=knn_object,\n",
    "        distance=max_object_dist,\n",
    "        overlap_relaxed=None,\n",
    "        corr_score=0.9,\n",
    "    )\n",
    "    scores = {\n",
    "        'region_recall_relaxed': len(matches['region_recall_relaxed']) / len(gt_regions),\n",
    "        'region_recall_strict': len(matches['region_recall_strict']) / len(gt_regions),\n",
    "        'region_precision_relaxed': len(matches['region_precision_relaxed']) / len(obs_regions),\n",
    "        'region_precision_strict': len(matches['region_precision_strict']) / len(obs_regions),\n",
    "        'object_recall': len(matches['object_recall']) / len(gt_objects),\n",
    "        'object_precision': len(matches['object_precision']) / len(obs_objects),\n",
    "    }\n",
    "    return matches, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ce2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_minAreaRect(img, rect):\n",
    "    box = cv2.boxPoints(rect)\n",
    "    box = np.intp(box)\n",
    "    # cv2.drawContours(img, [box], 0, (0, 0, 255), 2)\n",
    "    width = int(rect[1][0])\n",
    "    height = int(rect[1][1])\n",
    "    src_pts = box.astype(\"float32\")\n",
    "    dst_pts = np.array([[0, height-1],\n",
    "                        [0, 0],\n",
    "                        [width-1, 0],\n",
    "                        [width-1, height-1]], dtype=\"float32\")\n",
    "    M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
    "    warped = cv2.warpPerspective(img, M, (width, height))\n",
    "    return warped\n",
    "\n",
    "def check_visibility(center_i, center_j, wall_map):\n",
    "    rect = cv2.minAreaRect(np.array([center_i, center_j]).astype(np.int32))\n",
    "    rect = (rect[0], (max(rect[1][0], 2), max(rect[1][1], 10)), rect[2])\n",
    "    wall = crop_minAreaRect(wall_map.T, rect)\n",
    "    wall_pixels = wall.sum()\n",
    "    return wall_pixels\n",
    "\n",
    "def plot_visible_edge(fig, center_i, center_j, text, map_size=480):\n",
    "    if fig is None:\n",
    "        return\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[center_i[1], center_j[1]],\n",
    "            y=[map_size-center_i[0], map_size-center_j[0]],\n",
    "            mode='markers+lines',\n",
    "            marker=dict(size=6, color='red'),\n",
    "            line=dict(color='orange', width=3),\n",
    "            text=text,\n",
    "        )\n",
    "    )\n",
    "def plot_object(fig, center_i, text, map_size=480):\n",
    "    if fig is None:\n",
    "        return\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[center_i[1]],\n",
    "            y=[map_size-center_i[0]],\n",
    "            mode='markers',\n",
    "            marker=dict(size=6, color='blue'),\n",
    "            line=dict(color='pink', width=3),\n",
    "            text=text,\n",
    "        )\n",
    "    )\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, items):\n",
    "        self.parent = {i: i for i in items}\n",
    "        self.rank   = {i: 0 for i in items}\n",
    "\n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra == rb:\n",
    "            return\n",
    "        if self.rank[ra] < self.rank[rb]:\n",
    "            ra, rb = rb, ra\n",
    "        self.parent[rb] = ra\n",
    "        if self.rank[ra] == self.rank[rb]:\n",
    "            self.rank[ra] += 1\n",
    "from collections import defaultdict\n",
    "def detect_regions(objects, wall_map, knn_object=10, distance_threshold=30, wall_threshold=15, min_num=0, vis_fig=None):\n",
    "    # 1) Build id->object map and collect all ids\n",
    "    obj_map = {obj['id']: obj for obj in objects}\n",
    "    ids = list(obj_map.keys())\n",
    "\n",
    "    # 2) Initialize Union-Find on all ids\n",
    "    uf = UnionFind(ids)\n",
    "\n",
    "    # 3) For each object, query its nearby neighbors\n",
    "    object_tree = KDTree([x['center'] for x in objects])\n",
    "    for id_i in ids:\n",
    "        obj_i = obj_map[id_i]\n",
    "        # look for all neighbors\n",
    "        distances, indices = object_tree.query(obj_i['center'], k=knn_object)\n",
    "        if not isinstance(distances, np.ndarray):\n",
    "            distances, indices = np.array([distances]), np.array([indices])\n",
    "        else:\n",
    "            distances, indices = distances[:len(objects)], indices[:len(objects)]\n",
    "        for dist, j in zip(list(distances), list(indices)):\n",
    "            if dist > distance_threshold:\n",
    "                continue\n",
    "            obj_j = objects[j]\n",
    "            id_j = obj_j['id']\n",
    "            center_i = obj_i['center']\n",
    "            center_j = obj_j['center']\n",
    "            wall_pixels = check_visibility(center_i, center_j, wall_map)\n",
    "            if wall_pixels<=wall_threshold:\n",
    "                uf.union(id_i, id_j)\n",
    "                plot_visible_edge(vis_fig, center_i, center_j, f'{obj_i[\"caption\"]} - {obj_j[\"caption\"]}\\nwall_pixels={wall_pixels}', map_size=wall_map.shape[0])\n",
    "            else:\n",
    "                plot_object(vis_fig, center_i, f'{obj_i[\"caption\"]}\\nwall_pixels={wall_pixels}', map_size=wall_map.shape[0])\n",
    "    # 4) Bucket by root parent to form regions\n",
    "    groups = {}\n",
    "    for id in ids:\n",
    "        root = uf.find(id)\n",
    "        groups.setdefault(root,[]).append(obj_map[id])\n",
    "    sg_regions = []\n",
    "    for i, (_, objects) in enumerate(groups.items()):\n",
    "        if min_num > 0 and len(objects) < min_num:\n",
    "            continue\n",
    "        sg_regions.append({\n",
    "            'id': i,\n",
    "            'objects': objects,\n",
    "            'center': np.mean([obj['center'] for obj in objects], axis=0).tolist(),\n",
    "            'caption': 'unknown',\n",
    "        })\n",
    "    return sg_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943119ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6s7QHgap2fW_55 switch floor error\n",
    "# wcojb4TFT35_4 shift error\n",
    "\n",
    "episode_label = steps_df['episode'].iloc[3]\n",
    "step = 490\n",
    "data = get_sg_data(episode_label, step)\n",
    "\n",
    "obs_sg = PredictSceneGraph(data['cate_object'], clip_model_list=clip_model_list)\n",
    "obs_sg.init_from_json(data['global_scene_graph'])\n",
    "gt_sg = PredictSceneGraph(data['cate_object'], clip_model_list=clip_model_list)\n",
    "gt_sg.init_from_json(data['gt_scenegraph'])\n",
    "\n",
    "origins_grid = data['origins_grid']\n",
    "grid_size = args.map_resolution\n",
    "camera_position = data['camera_position'][:3, 3]\n",
    "# camera_position = np.array(data['camera_position']).reshape(4,4)[:3, 3]\n",
    "map_size = data['global_bev_rgb_map'].shape[0]\n",
    "\n",
    "\n",
    "floor_avg_heights = [floor['floor_avg_height'] for floor in gt_sg.scene_graph['floors']]\n",
    "floor_id = np.argmin(np.abs(np.array(floor_avg_heights) - camera_position[1]))\n",
    "\n",
    "print('processing gt regions')\n",
    "gt_objects = []\n",
    "gt_regions = gt_sg.scene_graph['floors'][floor_id]['regions']\n",
    "for region in gt_regions:\n",
    "    update_region(region, grid_size, origins_grid)\n",
    "    gt_objects.extend(region.get('objects'))\n",
    "\n",
    "print('processing obs regions')\n",
    "obs_regions = []\n",
    "obs_objects = []\n",
    "for room in obs_sg.scene_graph.get('rooms'):\n",
    "    if len(room.get('regions')) == 0:\n",
    "        continue\n",
    "    for region in room.get('regions'):\n",
    "        update_region(region, grid_size, origins_grid)\n",
    "        obs_regions.append(region)\n",
    "        obs_objects.extend(region.get('objects'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b667fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches, scores = evaluate_sg(\n",
    "    predict_sg=obs_sg,\n",
    "    obs_regions=obs_regions,\n",
    "    gt_regions=gt_regions,\n",
    "    obs_objects=obs_objects,\n",
    "    gt_objects=gt_objects,\n",
    "    knn_region=3,\n",
    "    knn_object=5,\n",
    "    max_object_dist=100.0/grid_size,\n",
    ")\n",
    "for k, v in scores.items():\n",
    "    print(f'{k}: {v:.2%}')\n",
    "\n",
    "obs_vis = create_fig(img=data['global_bev_rgb_map'][...,::-1])\n",
    "obs_vis = plot_region(obs_vis, obs_regions, map_size, show_objects=True)\n",
    "\n",
    "gt_vis = create_fig(img=data['global_bev_rgb_map'][...,::-1])\n",
    "gt_vis = plot_region(gt_vis, gt_regions, map_size, show_objects=True)\n",
    "\n",
    "plot_matches(obs_vis, gt_vis, matches['region_recall_relaxed']).show()\n",
    "plot_matches(obs_vis, gt_vis, matches['region_precision_relaxed'], reversed=True).show()\n",
    "# plot_matches(obs_vis, gt_vis, matches['region_recall_strict']).show()\n",
    "# plot_matches(obs_vis, gt_vis, matches['region_precision_strict'], reversed=True).show()\n",
    "# plot_matches(obs_vis, gt_vis, matches['object_recall']).show()\n",
    "# plot_matches(obs_vis, gt_vis, matches['object_precision'], reversed=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d49c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "objects = copy.deepcopy(obs_objects)\n",
    "bev_map = data['global_bev_rgb_map']\n",
    "gradient_map = data['gradient_map']\n",
    "wall_map = (gradient_map>1.2).astype(np.uint8)\n",
    "\n",
    "# detect rooms\n",
    "vis_fig = px.imshow(wall_map[::-1])\n",
    "new_rooms = detect_regions(objects, wall_map, knn_object=30, distance_threshold=50, wall_threshold=10, min_num=3, vis_fig=vis_fig)\n",
    "# vis_fig.show()\n",
    "for region in new_rooms:\n",
    "    update_region(region, grid_size, origins_grid)\n",
    "# room_vis = create_fig(img=bev_map[...,::-1])\n",
    "room_vis = px.imshow(wall_map[::-1])\n",
    "plot_region(room_vis, new_rooms, map_size, show_objects=True).show()\n",
    "\n",
    "# detect regions in the whole map\n",
    "# vis_fig = px.imshow(wall_map[::-1])\n",
    "# new_regions = detect_regions(objects, wall_map, knn_object=15, distance_threshold=30, wall_threshold=10, min_num=3,vis_fig=vis_fig)\n",
    "# vis_fig.show()\n",
    "# for region in new_regions:\n",
    "#     update_region(region, grid_size, origins_grid)\n",
    "# region_vis = create_fig(img=data['global_bev_rgb_map'][...,::-1])\n",
    "# plot_region(region_vis, new_regions, map_size, show_objects=True).show()\n",
    "\n",
    "# Rebuild the scene graph!!!\n",
    "# detect regions in each room and update the room\n",
    "new_sg = copy.deepcopy(new_rooms)\n",
    "new_regions = []\n",
    "new_objects = []\n",
    "for room in new_sg:\n",
    "    regions = detect_regions(room['objects'], wall_map, knn_object=15, distance_threshold=30, wall_threshold=10, min_num=3,vis_fig=vis_fig)\n",
    "    for region in regions:\n",
    "        update_region(region, grid_size, origins_grid)\n",
    "        new_objects.extend(region['objects'])\n",
    "    # update region and objects\n",
    "    room['regions'] = regions\n",
    "    new_regions.extend(regions)\n",
    "    room.pop('objects')\n",
    "    # update id\n",
    "    room['id'] = f\"{room['id']}\"\n",
    "    for region in regions:\n",
    "        region['id'] = f\"{room['id']}.{region['id']}\"\n",
    "        for obj in region['objects']:\n",
    "            obj['id'] = f\"{room['id']}.{region['id']}.{obj['id']}\"\n",
    "region_vis = px.imshow(wall_map[::-1])\n",
    "plot_region(region_vis, new_regions, map_size, show_objects=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9383cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to new_rooms to compare rooms, new_regions to compare regions\n",
    "new_obs_regions = new_regions\n",
    "\n",
    "\n",
    "matches, scores = evaluate_sg(\n",
    "    predict_sg=obs_sg,\n",
    "    obs_regions=new_obs_regions,\n",
    "    gt_regions=gt_regions,\n",
    "    obs_objects=new_objects,\n",
    "    gt_objects=gt_objects,\n",
    "    knn_region=3,\n",
    "    knn_object=5,\n",
    "    max_object_dist=100.0/grid_size,\n",
    ")\n",
    "for k, v in scores.items():\n",
    "    print(f'{k}: {v:.2%}')\n",
    "\n",
    "obs_vis = create_fig(img=data['global_bev_rgb_map'][...,::-1])\n",
    "obs_vis = plot_region(obs_vis, new_obs_regions, map_size, show_objects=True)\n",
    "\n",
    "gt_vis = create_fig(img=data['global_bev_rgb_map'][...,::-1])\n",
    "gt_vis = plot_region(gt_vis, gt_regions, map_size, show_objects=True)\n",
    "\n",
    "plot_matches(obs_vis, gt_vis, matches['region_recall_relaxed']).show()\n",
    "plot_matches(obs_vis, gt_vis, matches['region_precision_relaxed'], reversed=True).show()\n",
    "# plot_matches(obs_vis, gt_vis, matches['region_recall_strict']).show()\n",
    "# plot_matches(obs_vis, gt_vis, matches['region_precision_strict'], reversed=True).show()\n",
    "# plot_matches(obs_vis, gt_vis, matches['object_recall']).show()\n",
    "# plot_matches(obs_vis, gt_vis, matches['object_precision'], reversed=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(bev_map)\n",
    "show_image(gradient_map)\n",
    "show_image(wall_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_sg.scene_graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

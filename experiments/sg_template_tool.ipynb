{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "import types\n",
    "import open_clip\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from scipy.spatial import KDTree\n",
    "import peek\n",
    "import tqdm\n",
    "\n",
    "import sys\n",
    "from utils.db_utils import get_df, get_data, connect_db, DB\n",
    "from utils.plotly_utils import *\n",
    "from utils.vis import *\n",
    "from utils.predict_scenegraph import PredictSceneGraph\n",
    "from utils.imagine_nav_planner import ImagineNavPlanner\n",
    "from utils.scene_graph_utils import update_region\n",
    "from experiments.test_scenegraph_offline import evaluate_sg\n",
    "from constants import *\n",
    "\n",
    "dump_folder = './dump/prediction_may13/'\n",
    "output_folder = f'{dump_folder}/objectnav-dino'\n",
    "\n",
    "# load results\n",
    "print(f'Loading results from {output_folder}/result.db')\n",
    "args = types.SimpleNamespace(**json.load(open(f'{dump_folder}/args.json')))\n",
    "results = get_df(f'{output_folder}/result.db', 'result')\n",
    "print(f'Loaded {len(results)} results')\n",
    "print(f'Current success rate: {results.tail(1)[\"success\"].values[0]/len(results):.2%}')\n",
    "print(f'Current SPL: {results[\"spl\"].mean():.2f}')\n",
    "\n",
    "# load agent modules\n",
    "print(f'Loading agent modules')\n",
    "device = torch.device(\"cuda\")\n",
    "args = types.SimpleNamespace(**json.load(open(f'{dump_folder}/args.json')))\n",
    "with open(f'{dump_folder}/{args.exp_config}') as f:\n",
    "    exp_config = yaml.safe_load(f)\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n",
    "    \"ViT-H-14\", \"laion2b_s32b_b79k\"\n",
    ")\n",
    "clip_model = clip_model.to(device).half()\n",
    "clip_tokenizer = open_clip.get_tokenizer(\"ViT-H-14\")\n",
    "clip_model_list = (clip_model, clip_preprocess, clip_tokenizer)\n",
    "\n",
    "# access db\n",
    "import os\n",
    "import pathlib\n",
    "with DB(f'{output_folder}/result.db') as con:\n",
    "    table = con.table('result')\n",
    "    print(table)\n",
    "# episode infos\n",
    "# steps_df = get_df(f'{output_folder}/result.db', 'result', select=['count_steps', 'episode', 'target', 'habitat_success', 'switch_upstair_count', 'switch_downstair_count'])\n",
    "steps_df = get_df(f'{output_folder}/result.db', 'result', select=['count_steps', 'episode', 'target', 'habitat_success', 'switch_upstair_count', 'switch_downstair_count'])\n",
    "# print(steps_df.head(30))\n",
    "# step infos\n",
    "sample_episode_label = steps_df['episode'].values[0]\n",
    "with DB(f'{output_folder}/steps/{sample_episode_label}.db') as con:\n",
    "    table = con.table('step_data')\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sg_data(episode_label, step=None):\n",
    "    if step is None:\n",
    "        filter = lambda x: (x['episode_label']==episode_label) & (x['step']%5==0)\n",
    "    else:\n",
    "        filter = lambda x: (x['episode_label']==episode_label)\n",
    "    try:\n",
    "        data = get_data(\n",
    "            f'{output_folder}/steps/{episode_label}.db',\n",
    "            'step_data',\n",
    "            filter=filter,\n",
    "            select=[\n",
    "                'timestamp',\n",
    "                'step',\n",
    "                'episode_label',\n",
    "                'cate_object',\n",
    "                'origins_grid',\n",
    "                'current_grid_pose',\n",
    "                'camera_position_tensor',\n",
    "                'global_scene_graph_pickle',\n",
    "                'gt_scenegraph',\n",
    "                'predicted_global_scene_graph_pickle',\n",
    "                'global_bev_rgb_map_tensor',\n",
    "                # fbe\n",
    "                'traversible_map_tensor',\n",
    "                'occupancy_map_tensor',\n",
    "                'frontier_candidate_list',\n",
    "            ]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    # data = data[np.argmax([x['timestamp'] for x in data])]\n",
    "    return data\n",
    "\n",
    "def check_step(step):\n",
    "    keys = ['gt_scenegraph', 'global_scene_graph']\n",
    "    for key in keys:\n",
    "        if key not in step or step[key] is None:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "args = types.SimpleNamespace(**json.load(open(f'{dump_folder}/args.json')))\n",
    "\n",
    "count_episode = 0\n",
    "valid_steps = []\n",
    "for i in range(len(results)):\n",
    "    episode_label = results['episode'].iloc[i]\n",
    "    print(f'Processing episode {episode_label}')\n",
    "    data = get_sg_data(episode_label)\n",
    "    if data is None:\n",
    "        # print(f'No data for episode {episode_label}')\n",
    "        continue\n",
    "    count_episode += 1\n",
    "    # find the last step with valid sg\n",
    "    for step in data[::-1]:\n",
    "        if check_step(step):\n",
    "            valid_steps.append(step)\n",
    "            break\n",
    "    if len(valid_steps) >= 5:\n",
    "        break\n",
    "print(f'Loaded {count_episode} episodes')\n",
    "print(f'Loaded {len(valid_steps)} sg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GPT_API_KEY\"] = \"sk-Sg58hfwiUGtMak37De998909A76c44E88f7dF36730Dd29B4\"\n",
    "step = valid_steps[4]\n",
    "print(f'episode {step[\"episode_label\"]} step {step[\"step\"]}')\n",
    "grid_size = args.map_resolution\n",
    "map_size = args.map_size_cm/grid_size\n",
    "origins_grid = step['origins_grid']\n",
    "camera_position = step['camera_position'][:3, 3]\n",
    "target = step['cate_object']\n",
    "\n",
    "obs_sg = step['global_scene_graph']\n",
    "obs_regions = [update_region(region, grid_size, origins_grid) for room in obs_sg['rooms'] for region in room['regions']]\n",
    "obs_objects = [obj for region in obs_regions for obj in region['objects']]\n",
    "\n",
    "\n",
    "gt_sg = step['gt_scenegraph']\n",
    "floor_avg_heights = [floor['floor_avg_height'] for floor in gt_sg['floors']]\n",
    "floor_id = np.argmin(np.abs(np.array(floor_avg_heights) - camera_position[1]))\n",
    "for room in gt_sg['floors']:\n",
    "    for region in room['regions']:\n",
    "        region['caption'] = gt_region_captions_map[region['caption']]\n",
    "gt_regions = [update_region(region, grid_size, origins_grid) for region in gt_sg['floors'][floor_id]['regions']]\n",
    "gt_objects = [obj for region in gt_regions for obj in region['objects']]\n",
    "\n",
    "# fbe\n",
    "frontier_candidate_list = step['frontier_candidate_list']\n",
    "current_grid_pose = step['current_grid_pose']\n",
    "traversible_map = step['traversible_map']\n",
    "occupancy_map = step['occupancy_map']\n",
    "global_bev_rgb_map = step['global_bev_rgb_map']\n",
    "\n",
    "imagine_nav_planner = ImagineNavPlanner(args, exp_config, clip_model_list)\n",
    "scene_graph = imagine_nav_planner.scene_graph\n",
    "imagine_nav_planner.origins_grid = origins_grid\n",
    "\n",
    "\n",
    "scene_graph.grid_size = grid_size\n",
    "scene_graph.scene_graph = copy.deepcopy(obs_sg)\n",
    "imagine_nav_planner.set_obj_goal(target)\n",
    "imagine_nav_planner.set_step(step[\"step\"], episode_label)\n",
    "scores, best_frontier_id, exploration_scores = imagine_nav_planner.fbe(\n",
    "    frontier_candidate_list,\n",
    "    current_grid_pose,\n",
    "    traversible_map,\n",
    "    occupancy_map,\n",
    "    global_bev_rgb_map,\n",
    "    gt_sg['floors'][floor_id],\n",
    ")\n",
    "\n",
    "pred_sg = imagine_nav_planner.predicted_global_scene_graph\n",
    "pred_regions = [update_region(region, grid_size, origins_grid) for room in pred_sg['rooms'] for region in room['regions']]\n",
    "pred_objects = [obj for region in pred_regions for obj in region['objects']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peek(scene_graph._llm_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scene_graph._response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning = [f'{region[\"id\"]} {region[\"caption\"]}: {region[\"reasoning\"]}' for room in scene_graph.predicted_sg['rooms'] for region in room['regions']]\n",
    "reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peek(target)\n",
    "\n",
    "vis = create_fig(img=step['global_bev_rgb_map'][...,::-1])\n",
    "vis.update_layout(width=1000, height=1000)\n",
    "vis = plot_region(vis, pred_regions, map_size, show_objects=True, min_region_size=1)\n",
    "vis.show()\n",
    "\n",
    "gt_vis = create_fig(img=step['global_bev_rgb_map'][...,::-1])\n",
    "gt_vis.update_layout(width=1000, height=1000)\n",
    "gt_vis = plot_region(gt_vis, gt_regions, map_size, show_objects=True, min_region_size=1)\n",
    "gt_vis.show()\n",
    "\n",
    "from utils.vis import remove_image_border\n",
    "peek(scene_graph.obs_bev_image.shape)\n",
    "vlm_fig = create_fig(img=scene_graph.obs_bev_image)\n",
    "vlm_fig.update_layout(width=1000, height=1000)\n",
    "vlm_fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagine_nav_planner.semantic_graph_based(pred_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in pred_regions:\n",
    "    print(region['id'], region.get('predicted', False))\n",
    "    print(region['caption'])\n",
    "    print(region['corr_score'])\n",
    "    print('-'*20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge regions\n",
    "# from utils.scene_graph_utils import update_region, detect_match, UnionFind\n",
    "# pred_regions = merge_regions(obs_regions)\n",
    "# print(len(obs_regions), len(pred_regions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs_matches, obs_scores = evaluate_sg(\n",
    "#     clip_model_list=imagine_nav_planner.clip_model_list,\n",
    "#     obs_regions=obs_regions,\n",
    "#     gt_regions=gt_regions,\n",
    "#     obs_objects=obs_objects,\n",
    "#     gt_objects=gt_objects,\n",
    "#     knn_region=3,\n",
    "#     knn_object=5,\n",
    "#     max_object_dist=100.0/grid_size,\n",
    "# )\n",
    "# pred_matches, pred_scores = evaluate_sg(\n",
    "#     clip_model_list=imagine_nav_planner.clip_model_list,\n",
    "#     obs_regions=pred_regions,\n",
    "#     gt_regions=gt_regions,\n",
    "#     obs_objects=pred_objects,\n",
    "#     gt_objects=gt_objects,\n",
    "#     knn_region=3,\n",
    "#     knn_object=5,\n",
    "#     max_object_dist=100.0/grid_size,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_vis = create_fig(img=step['global_bev_rgb_map'][...,::-1])\n",
    "obs_vis = plot_region(obs_vis, obs_regions, map_size, show_objects=True, min_region_size=1)\n",
    "\n",
    "pred_vis = create_fig(img=step['global_bev_rgb_map'][...,::-1])\n",
    "pred_vis = plot_region(pred_vis, pred_regions, map_size, show_objects=True, min_region_size=1)\n",
    "\n",
    "gt_vis = create_fig(img=step['global_bev_rgb_map'][...,::-1])\n",
    "gt_vis = plot_region(gt_vis, gt_regions, map_size, show_objects=True, min_region_size=1)\n",
    "\n",
    "obs_vis.show()\n",
    "pred_vis.show()\n",
    "gt_vis.show()\n",
    "\n",
    "# plot_matches(obs_vis, gt_vis, matches['region_recall_relaxed']).show()\n",
    "# plot_matches(obs_vis, gt_vis, matches['region_precision_relaxed'], reversed=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[35]\n",
      "[35, 40, 50]\n",
      "[30, 35, 40, 45]\n",
      "[30, 35, 40, 45, 50]\n",
      "[30, 35, 40, 45, 50]\n",
      "[30, 45, 60, 75, 90]\n",
      "[30, 70, 110, 150, 190]\n"
     ]
    }
   ],
   "source": [
    "def sample_steps(step_list, n=5):\n",
    "    step_list = [x for x in step_list if x>=30]\n",
    "    if len(step_list) <= n:\n",
    "        return step_list\n",
    "    else:\n",
    "        # Calculate interval to get n evenly spaced samples\n",
    "        interval = max(1, (len(step_list)-1) // (n-1))\n",
    "        # Return evenly spaced samples using the interval\n",
    "        return step_list[::interval][:n]\n",
    "print(sample_steps([5,10]))\n",
    "print(sample_steps([5,10,35]))\n",
    "print(sample_steps([5,10,35,40,50]))\n",
    "print(sample_steps([*range(0,50,5)]))\n",
    "print(sample_steps([*range(0,55,5)]))\n",
    "print(sample_steps([*range(0,60,5)]))\n",
    "print(sample_steps([*range(0,100,5)]))\n",
    "print(sample_steps([*range(0,200,5)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
